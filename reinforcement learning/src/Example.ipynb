{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuantumClifford\n",
    "using QuantumClifford.Experimental.NoisyCircuits\n",
    "using Random\n",
    "using AbstractAlgebra\n",
    "using Quantikz: displaycircuit\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using DataFrames\n",
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Flux.Losses: huber_loss\n",
    "using ClosedIntervals\n",
    "using Zygote\n",
    "using ComponentArrays\n",
    "using StableRNGs\n",
    "using QuantumClifford.Experimental.NoisyCircuits: applyop!, affectedqubits, applyop_branches\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d700efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"environment.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea164e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=CircuitEnv(\n",
    "        initial_pairs = 3,\n",
    "        net_noise = 0.1,\n",
    "        local_noise = 0.01,\n",
    "        entanglement_type = :bell,\n",
    "        max_len=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function bandit_testbed(\n",
    "    ;max_len=5,\n",
    "    initial_pairs=3,\n",
    "    warmup_steps = 20,\n",
    "    δ = 0.85,\n",
    "    ϵ_stable = 0.0001,\n",
    "    kind = :linear, # :exp or :linear\n",
    "    decay_steps = 1000,\n",
    "    stop_after_episode = 1000,\n",
    "    batch_size = 32,\n",
    "    trajectories = 500,\n",
    "    optimizer = ADAM()\n",
    ")\n",
    "    env = CircuitEnv(;max_len=max_len,initial_pairs=initial_pairs,trajectories = trajectories)\n",
    "    ns, na = length(RLBase.state(env)), length(action_space(env))\n",
    "    agent = Agent(\n",
    "               policy = QBasedPolicy(\n",
    "                   learner = BasicDQNLearner(\n",
    "                       approximator = NeuralNetworkApproximator(\n",
    "                           model = Chain(\n",
    "                               Dense(ns, 128, relu; init = glorot_uniform),\n",
    "                               Dense(128, 128, relu; init = glorot_uniform),\n",
    "                               Dense(128, na; init = glorot_uniform),\n",
    "                           ) |> cpu,\n",
    "                           optimizer = optimizer,\n",
    "                       ),\n",
    "                       batch_size = batch_size,\n",
    "                       min_replay_history = 100,\n",
    "                       loss_func = huber_loss,\n",
    "                   ),\n",
    "                   explorer = MyExplorer1(\n",
    "                       warmup_steps = warmup_steps,\n",
    "                       δ = δ,\n",
    "                       N = initial_pairs,\n",
    "                       kind = kind,\n",
    "                       ϵ_stable = ϵ_stable, # MODIFIED just a hunch, let's make it more risk-taking\n",
    "                       decay_steps = decay_steps,\n",
    "                   ),\n",
    "               ),\n",
    "               trajectory = CircularArraySARTTrajectory( # TODO is this a good choice?\n",
    "                   capacity = 500, # MODIFIED no idea, but probably not necessary to be that big\n",
    "                   state = Vector{Float32} => (ns,), # TODO what is this actually doing?\n",
    "                   action = Int => (),\n",
    "                   reward = Float32 => (),\n",
    "                   terminal = Bool => (),\n",
    "               ),\n",
    "           )\n",
    "    h1 = MyHook(;)\n",
    "    h2 = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "    h3 = RewardsPerEpisode()\n",
    "    h4 = StepsPerEpisode()\n",
    "    run(agent, env, StopAfterEpisode(stop_after_episode), ComposedHook(h1, h2, h3, h4))\n",
    "    return h1.fidelity, h2.rewards, h1.finalcircuit, agent\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f=DataFrame()\n",
    "df_r=DataFrame()\n",
    "for optimizer in [ADAM(),OADAM()]\n",
    "    for batch_size in [32,64]\n",
    "        for trajectories in [500,1000]\n",
    "            fidelity,reward,_,_=bandit_testbed(;\n",
    "                warmup_steps = 400,\n",
    "                decay_steps = 800, \n",
    "                stop_after_episode = 500,\n",
    "                batch_size = batch_size,\n",
    "                trajectories = trajectories,\n",
    "                optimizer = optimizer\n",
    "            )\n",
    "            df1=DataFrame(iteration=collect(1:length(fidelity)), fidelity=fidelity,\n",
    "                batch_size=fill(batch_size, length(fidelity)), \n",
    "                trajectories=fill(trajectories, length(fidelity)), \n",
    "                optimizer=fill(\"$optimizer\", length(fidelity)))\n",
    "            df2=DataFrame(iteration=collect(1:length(reward)), reward=reward,\n",
    "                batch_size=fill(batch_size, length(reward)), \n",
    "                trajectories=fill(trajectories, length(reward)), \n",
    "                optimizer=fill(\"$optimizer\", length(reward)))\n",
    "            append!(df_f,df1)\n",
    "            append!(df_r,df2)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "using StatsPlots\n",
    "@df df_f plot(:iteration, :fidelity, ylim=(0,0.8), legend=:bottomright, size=(1000,600), group=(:batch_size,:epsilon,:trajectories), layout=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@df df_r plot(:iteration, :reward, ylim=(-3,2), legend=:bottomright, xlabel=\"iteration\", ylabel=\"fidelity\", size=(1000,600), group=(:batch_size,:epsilon,:trajectories), layout=(2,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
